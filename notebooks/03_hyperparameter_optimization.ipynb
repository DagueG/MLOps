{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f3c0b40",
   "metadata": {},
   "source": [
    "# Etape 3 - Optimisation des Hyperparam√®tres\n",
    "\n",
    "Objectif: Optimiser les hyperparam√®tres des 2 meilleurs mod√®les (LightGBM et XGBoost) en utilisant GridSearchCV et Optuna avec MLflow tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e765735",
   "metadata": {},
   "source": [
    "## 1. Import et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c07ef2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root: C:\\Users\\daniel.guedj_arondor\\Downloads\\perso\\openclassrooms\\MLOps\n",
      "Data: C:\\Users\\daniel.guedj_arondor\\Downloads\\perso\\openclassrooms\\MLOps\\outputs\n",
      "Models: C:\\Users\\daniel.guedj_arondor\\Downloads\\perso\\openclassrooms\\MLOps\\models\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Paths\n",
    "ROOT_DIR = Path('.').resolve().parent\n",
    "DATA_DIR = ROOT_DIR / 'outputs'\n",
    "MODELS_DIR = ROOT_DIR / 'models'\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Root: {ROOT_DIR}\")\n",
    "print(f\"Data: {DATA_DIR}\")\n",
    "print(f\"Models: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a760d31e",
   "metadata": {},
   "source": [
    "## 2. Charger les donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00286518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des donnees...\n",
      "Train shape: (307511, 148)\n",
      "Test shape: (48744, 121)\n",
      "\n",
      "Distribution TARGET:\n",
      "TARGET\n",
      "0    282686\n",
      "1     24825\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Chargement des donnees...\")\n",
    "train = pd.read_csv(DATA_DIR / 'train_processed.csv')\n",
    "test = pd.read_csv(DATA_DIR / 'test_processed.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "# Verifier TARGET\n",
    "print(f\"\\nDistribution TARGET:\")\n",
    "print(train['TARGET'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d7df3",
   "metadata": {},
   "source": [
    "## 3. Pr√©parer les donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc4142da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (307511, 146)\n",
      "y_train shape: (307511,)\n",
      "X_test shape: (48744, 120)\n"
     ]
    }
   ],
   "source": [
    "# Separer X et y\n",
    "X_train = train.drop('TARGET', axis=1)\n",
    "y_train = train['TARGET']\n",
    "\n",
    "# Test\n",
    "if 'TARGET' in test.columns:\n",
    "    X_test = test.drop('TARGET', axis=1)\n",
    "    y_test = test['TARGET']\n",
    "else:\n",
    "    X_test = test.copy()\n",
    "    y_test = None\n",
    "\n",
    "# Garder les ids\n",
    "if 'SK_ID_CURR' in X_test.columns:\n",
    "    test_ids = X_test['SK_ID_CURR'].copy()\n",
    "    X_train = X_train.drop('SK_ID_CURR', axis=1, errors='ignore')\n",
    "    X_test = X_test.drop('SK_ID_CURR', axis=1)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe583c",
   "metadata": {},
   "source": [
    "## 4. Configuration MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54a39511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: credit_scoring_v1\n",
      "Experiment ID: 1\n"
     ]
    }
   ],
   "source": [
    "# Configurer MLflow\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "experiment_name = 'credit_scoring_v1'\n",
    "\n",
    "# Creer experiment si elle n'existe pas\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(experiment_name)\n",
    "except:\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    experiment_id = experiment.experiment_id\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "print(f\"Experiment: {experiment_name}\")\n",
    "print(f\"Experiment ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e04c43e",
   "metadata": {},
   "source": [
    "## 5. Fonction de m√©triques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48744132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonction de metriques: OK\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(y_true, y_pred, y_pred_proba=None, cost_fn=10, cost_fp=1):\n",
    "    \"\"\"\n",
    "    Calcule les metriques de classification.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Metriques standards\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['precision'] = precision_score(y_true, y_pred, zero_division=0)\n",
    "    metrics['recall'] = recall_score(y_true, y_pred, zero_division=0)\n",
    "    metrics['f1'] = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # AUC\n",
    "    if y_pred_proba is not None:\n",
    "        metrics['auc'] = roc_auc_score(y_true, y_pred_proba[:, 1])\n",
    "    \n",
    "    # Co√ªt m√©tier\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics['business_cost'] = fn * cost_fn + fp * cost_fp\n",
    "    metrics['tn'] = int(tn)\n",
    "    metrics['fp'] = int(fp)\n",
    "    metrics['fn'] = int(fn)\n",
    "    metrics['tp'] = int(tp)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"Fonction de metriques: OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b043300",
   "metadata": {},
   "source": [
    "## 6. GridSearchCV - LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe482582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GRIDSEARCHCV - LIGHTGBM ===\n",
      "Total combinations: 81\n",
      "Starting GridSearchCV...\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/09 13:48:44 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/02/09 13:48:53 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 150, 'num_leaves': 31}\n",
      "Best CV Score (AUC): 0.7699\n",
      "AUC: 0.7699\n",
      "Accuracy: 0.7230\n",
      "Business Cost: 157859\n",
      "Training time: 681.39s\n",
      "üèÉ View run lightgbm_gridsearch at: http://localhost:5000/#/experiments/1/runs/0e8fbdf54a8d45fc9083e0ec9579d281\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== GRIDSEARCHCV - LIGHTGBM ===\")\n",
    "\n",
    "# Hyperparametres a optimiser\n",
    "param_grid_lgb = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'num_leaves': [31, 50, 100]\n",
    "}\n",
    "\n",
    "scale_pos = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "base_model_lgb = lgb.LGBMClassifier(\n",
    "    scale_pos_weight=scale_pos,\n",
    "    random_state=42,\n",
    "    verbosity=-1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_lgb = GridSearchCV(\n",
    "    base_model_lgb,\n",
    "    param_grid_lgb,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"Total combinations: {len(param_grid_lgb['n_estimators']) * len(param_grid_lgb['max_depth']) * len(param_grid_lgb['learning_rate']) * len(param_grid_lgb['num_leaves'])}\")\n",
    "print(\"Starting GridSearchCV...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"lightgbm_gridsearch\"):\n",
    "    start_time = time.time()\n",
    "    grid_search_lgb.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    best_params_lgb = grid_search_lgb.best_params_\n",
    "    best_model_lgb = grid_search_lgb.best_estimator_\n",
    "    \n",
    "    # Predictions avec meilleur modele\n",
    "    y_pred_proba = cross_val_predict(best_model_lgb, X_train, y_train, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), method='predict_proba')\n",
    "    y_pred = cross_val_predict(best_model_lgb, X_train, y_train, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42))\n",
    "    \n",
    "    metrics = calculate_metrics(y_train, y_pred, y_pred_proba)\n",
    "    \n",
    "    # Log\n",
    "    mlflow.log_params(best_params_lgb)\n",
    "    mlflow.log_metrics(metrics)\n",
    "    mlflow.log_metric('gridsearch_best_cv_score', grid_search_lgb.best_score_)\n",
    "    mlflow.log_metric('training_time', training_time)\n",
    "    mlflow.sklearn.log_model(best_model_lgb, 'model')\n",
    "    \n",
    "    print(f\"\\nBest Params: {best_params_lgb}\")\n",
    "    print(f\"Best CV Score (AUC): {grid_search_lgb.best_score_:.4f}\")\n",
    "    print(f\"AUC: {metrics['auc']:.4f}\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Business Cost: {metrics['business_cost']:.0f}\")\n",
    "    print(f\"Training time: {training_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb09aa54",
   "metadata": {},
   "source": [
    "## 7. GridSearchCV - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b20de390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GRIDSEARCHCV - XGBOOST ===\n",
      "Total combinations: 81\n",
      "Starting GridSearchCV...\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/09 13:56:51 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/02/09 13:56:56 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Best CV Score (AUC): 0.7688\n",
      "AUC: 0.7686\n",
      "Accuracy: 0.7241\n",
      "Business Cost: 158555\n",
      "Training time: 441.03s\n",
      "üèÉ View run xgboost_gridsearch at: http://localhost:5000/#/experiments/1/runs/6b3af238e0fe4a3699bcc8d9f5f7d923\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== GRIDSEARCHCV - XGBOOST ===\")\n",
    "\n",
    "# Hyperparametres a optimiser\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "base_model_xgb = xgb.XGBClassifier(\n",
    "    scale_pos_weight=scale_pos,\n",
    "    random_state=42,\n",
    "    verbosity=0,\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    base_model_xgb,\n",
    "    param_grid_xgb,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"Total combinations: {len(param_grid_xgb['n_estimators']) * len(param_grid_xgb['max_depth']) * len(param_grid_xgb['learning_rate']) * len(param_grid_xgb['subsample'])}\")\n",
    "print(\"Starting GridSearchCV...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"xgboost_gridsearch\"):\n",
    "    start_time = time.time()\n",
    "    grid_search_xgb.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    best_params_xgb = grid_search_xgb.best_params_\n",
    "    best_model_xgb = grid_search_xgb.best_estimator_\n",
    "    \n",
    "    # Predictions avec meilleur modele\n",
    "    y_pred_proba = cross_val_predict(best_model_xgb, X_train, y_train, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), method='predict_proba')\n",
    "    y_pred = cross_val_predict(best_model_xgb, X_train, y_train, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42))\n",
    "    \n",
    "    metrics = calculate_metrics(y_train, y_pred, y_pred_proba)\n",
    "    \n",
    "    # Log\n",
    "    mlflow.log_params(best_params_xgb)\n",
    "    mlflow.log_metrics(metrics)\n",
    "    mlflow.log_metric('gridsearch_best_cv_score', grid_search_xgb.best_score_)\n",
    "    mlflow.log_metric('training_time', training_time)\n",
    "    mlflow.sklearn.log_model(best_model_xgb, 'model')\n",
    "    \n",
    "    print(f\"\\nBest Params: {best_params_xgb}\")\n",
    "    print(f\"Best CV Score (AUC): {grid_search_xgb.best_score_:.4f}\")\n",
    "    print(f\"AUC: {metrics['auc']:.4f}\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Business Cost: {metrics['business_cost']:.0f}\")\n",
    "    print(f\"Training time: {training_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b0276b",
   "metadata": {},
   "source": [
    "## 8. Comparaison avec baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dca1188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPARAISON: BASELINE vs OPTIMIZED ===\n",
      "\n",
      "Baseline runs: 4\n",
      "Optimized runs: 2\n",
      "\n",
      "--- BASELINE MODELS ---\n",
      "                       Model      AUC  Accuracy  Business Cost  CV AUC Mean\n",
      "           lightgbm_baseline 0.765671  0.713236       159859.0     0.765709\n",
      "            xgboost_baseline 0.765458  0.715265       159658.0     0.765482\n",
      "logistic_regression_baseline 0.757313  0.695026       163281.0     0.757339\n",
      "      random_forest_baseline 0.741323  0.738178       169766.0     0.741351\n",
      "\n",
      "--- OPTIMIZED MODELS ---\n",
      "              Model      AUC  Accuracy  Business Cost  Best CV Score\n",
      "lightgbm_gridsearch 0.769903  0.722989       157859.0       0.769922\n",
      " xgboost_gridsearch 0.768570  0.724091       158555.0       0.768817\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== COMPARAISON: BASELINE vs OPTIMIZED ===\")\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "all_runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "# Filtrer baseline et optimized\n",
    "baseline_runs = all_runs[all_runs['tags.mlflow.runName'].str.contains('baseline', case=False, na=False)]\n",
    "optimized_runs = all_runs[all_runs['tags.mlflow.runName'].str.contains('gridsearch', case=False, na=False)]\n",
    "\n",
    "print(f\"\\nBaseline runs: {len(baseline_runs)}\")\n",
    "print(f\"Optimized runs: {len(optimized_runs)}\")\n",
    "\n",
    "# Baseline\n",
    "if len(baseline_runs) > 0:\n",
    "    print(\"\\n--- BASELINE MODELS ---\")\n",
    "    baseline_comparison = baseline_runs[['tags.mlflow.runName', 'metrics.auc', 'metrics.accuracy', 'metrics.business_cost', 'metrics.cv_auc_mean']].copy()\n",
    "    baseline_comparison.columns = ['Model', 'AUC', 'Accuracy', 'Business Cost', 'CV AUC Mean']\n",
    "    baseline_comparison = baseline_comparison.sort_values('CV AUC Mean', ascending=False)\n",
    "    print(baseline_comparison.to_string(index=False))\n",
    "\n",
    "# Optimized\n",
    "if len(optimized_runs) > 0:\n",
    "    print(\"\\n--- OPTIMIZED MODELS ---\")\n",
    "    optimized_comparison = optimized_runs[['tags.mlflow.runName', 'metrics.auc', 'metrics.accuracy', 'metrics.business_cost', 'metrics.gridsearch_best_cv_score']].copy()\n",
    "    optimized_comparison.columns = ['Model', 'AUC', 'Accuracy', 'Business Cost', 'Best CV Score']\n",
    "    optimized_comparison = optimized_comparison.sort_values('AUC', ascending=False)\n",
    "    print(optimized_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee14f0c2",
   "metadata": {},
   "source": [
    "## Prochaines √©tapes\n",
    "\n",
    "- **√âtape 4**: Feature Importance (SHAP) et Optimisation du seuil de d√©cision\n",
    "- S√©lection du meilleur mod√®le optimis√©\n",
    "- Model Registry dans MLflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit-scoring-mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
